{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio\n",
    "# perform stft\n",
    "# plot stfts\n",
    "# spatiotemporal gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd  # Import pandas for CSV reading\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from brian2 import *\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Configuration Constants\n",
    "FILETYPES = (\".ogg\", \".mp3\", \".wav\")\n",
    "BASE_PATH = \"./data/audio_dataset/\"\n",
    "AUDIO_INPUT_TEST = os.path.join(BASE_PATH, \"test\")\n",
    "AUDIO_INPUT_TRAIN = os.path.join(BASE_PATH, \"train\")\n",
    "STFT_PLOTS = \"./data/stft_plots/\"\n",
    "SPATIOTEMPORAL_PLOTS = \"./data/spatiotemporal_plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_stft(audio: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert audio to Short-Time Fourier Transform (STFT).\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): Input audio time series\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Magnitude of STFT\n",
    "    \"\"\"\n",
    "    return np.abs(librosa.stft(audio))\n",
    "\n",
    "def read_audio_label(dir: str, filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract audio type tag from corresponding CSV file.\n",
    "\n",
    "    Args:\n",
    "        dir (str): Directory containing the CSV file\n",
    "        filename (str): Name of the audio file\n",
    "\n",
    "    Returns:\n",
    "        Optional label from the CSV file\n",
    "    \"\"\"\n",
    "    csv_filename = os.path.splitext(filename)[0] + \".csv\"\n",
    "    csv_filepath = os.path.join(dir, csv_filename)\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_filepath, names=[\"startTime\", \"endTime\", \"quantity\", \"label\"]\n",
    "        )\n",
    "        return df.iloc[0, 3]  # label in 4th column\n",
    "    except (FileNotFoundError, IndexError) as e:\n",
    "        print(f\"Warning: Unable to read label for {csv_filename}. Error: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "def process_audio_dataset(\n",
    "    data_path: str, \n",
    "    filetypes: Tuple[str, ...]\n",
    ") -> Tuple[\n",
    "    Dict[str, np.ndarray],  # audio dictionary \n",
    "    Dict[str, int],          # sampling rate dictionary\n",
    "    Dict[str, np.ndarray],  # STFT data dictionary\n",
    "    Dict[str, Optional[str]]  # audio labels dictionary\n",
    "]:\n",
    "    \"\"\"\n",
    "    Process audio files in a given directory.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing audio files\n",
    "        filetypes (Tuple[str, ...]): Allowed audio file extensions\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of dictionaries containing:\n",
    "        - Audio time series\n",
    "        - Sampling rates\n",
    "        - STFT data\n",
    "        - Audio labels\n",
    "    \"\"\"\n",
    "    stft_data = {}\n",
    "    audio_labels = {}\n",
    "    sr = {}\n",
    "    audio = {}\n",
    "    \n",
    "    for filename in os.listdir(data_path):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        \n",
    "        if file_path.endswith(filetypes):\n",
    "            try:\n",
    "                # Load audio\n",
    "                audio[filename], sr[filename] = librosa.load(file_path, sr=None)\n",
    "                \n",
    "                # Compute STFT\n",
    "                stft_data[filename] = np.abs(librosa.stft(audio[filename]))\n",
    "                \n",
    "                # Get label\n",
    "                audio_labels[filename] = read_audio_label(data_path, filename)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return audio, sr, stft_data, audio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_audio_stft(audio, sr, stft, output_filename):\n",
    "    \"\"\"\n",
    "    Plots the waveform and STFT of a single audio file and saves the plot.\n",
    "\n",
    "    Args:\n",
    "        stft (np.ndarray): The STFT matrix of the audio file.\n",
    "        output_filename (str): The name of the output file for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create a figure with two rows and shared x-axis\n",
    "    plt.plot(figsize=(12, 6))\n",
    "\n",
    "\n",
    "    # Plot the STFT in the second row\n",
    "    librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(stft, ref=np.max),\n",
    "        sr=sr,\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(STFT_PLOTS, f\"{output_filename}.png\"), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatiotemporal_gen(stft_plot_path, audio_data, sr, output_filename):\n",
    "    \"\"\"\n",
    "    Generates spatiotemporal plots using the STFT plot and audio data.\n",
    "\n",
    "    Args:\n",
    "        stft_plot_path (str): Path to the STFT plot image.\n",
    "        audio_data (np.ndarray): The audio data.\n",
    "        sr (int): The sample rate of the audio data.\n",
    "        output_filename (str): The name of the output file for the spatiotemporal plot.\n",
    "    \"\"\"\n",
    "    # Verify that the file exists\n",
    "    if not os.path.exists(stft_plot_path):\n",
    "        raise FileNotFoundError(f\"The file {stft_plot_path} does not exist.\")\n",
    "\n",
    "    # Load the chirplet transform spectrogram PNG image using OpenCV\n",
    "    duration = librosa.get_duration(y=audio_data, sr=sr)\n",
    "    image = cv2.imread(stft_plot_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image at {stft_plot_path}. Please check the file.\")\n",
    "\n",
    "    image_data = image.astype(float) / 255.0\n",
    "    height, width = image_data.shape\n",
    "\n",
    "    time_duration = duration * second  # Use the actual audio duration\n",
    "    num_neurons = height\n",
    "    global_threshold = 0.09\n",
    "    block_size = 19\n",
    "    C = -5\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(\n",
    "        (image_data * 255).astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, C\n",
    "    )\n",
    "    adaptive_thresh = adaptive_thresh / 255.0\n",
    "\n",
    "    times = []\n",
    "    neurons = []\n",
    "    for ex in range(width):  # ex and why used to not conflict with x and y\n",
    "        for why in range(height):\n",
    "            if image_data[why, ex] > global_threshold and adaptive_thresh[why, ex] > 0:\n",
    "                time_point = ex / width * duration\n",
    "                neuron_point = why\n",
    "                times.append(time_point)\n",
    "                neurons.append(neuron_point)\n",
    "\n",
    "    times = np.array(times) * second\n",
    "    neurons = np.array(neurons, dtype=int)\n",
    "\n",
    "    G = NeuronGroup(num_neurons, 'v : 1', threshold='v>1', reset='v=0', method='exact')\n",
    "    G.v = 0\n",
    "    input_group = SpikeGeneratorGroup(num_neurons, indices=neurons, times=times)\n",
    "    S = Synapses(input_group, G, on_pre='v_post += 1')\n",
    "    S.connect(j='i')\n",
    "    run(time_duration)\n",
    "\n",
    "    # Create the spatiotemporal plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(times / second, neurons, s=1, c='blue', alpha=0.6)\n",
    "    plt.title(\"Spatiotemporal Activity\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Neuron Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SPATIOTEMPORAL_PLOTS, f\"{output_filename}.png\"), bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m train_audio, train_sr, train_stft_data, train_audio_labels \u001b[38;5;241m=\u001b[39m process_audio_dataset(\n\u001b[1;32m     16\u001b[0m     AUDIO_INPUT_TRAIN, \n\u001b[1;32m     17\u001b[0m     FILETYPES\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Process test dataset\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m test_audio, test_sr, test_stft_data, test_audio_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mAUDIO_INPUT_TEST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFILETYPES\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_stft_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train audio files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mprocess_audio_dataset\u001b[0;34m(data_path, filetypes)\u001b[0m\n\u001b[1;32m     40\u001b[0m audio[filename], sr[filename] \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Compute STFT\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m stft_data[filename] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Get label\u001b[39;00m\n\u001b[1;32m     46\u001b[0m audio_labels[filename] \u001b[38;5;241m=\u001b[39m read_audio_label(data_path, filename)\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/librosa/core/spectrum.py:387\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[1;32m    385\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 387\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/numpy/fft/_pocketfft.py:409\u001b[0m, in \u001b[0;36mrfft\u001b[0;34m(a, n, axis, norm)\u001b[0m\n\u001b[1;32m    407\u001b[0m     n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m    408\u001b[0m inv_norm \u001b[38;5;241m=\u001b[39m _get_forward_norm(n, norm)\n\u001b[0;32m--> 409\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/numpy/fft/_pocketfft.py:73\u001b[0m, in \u001b[0;36m_raw_fft\u001b[0;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     a \u001b[38;5;241m=\u001b[39m swapaxes(a, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mpfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     r \u001b[38;5;241m=\u001b[39m swapaxes(r, axis, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to process train and test audio datasets\n",
    "\"\"\"\n",
    "\n",
    "directories = [AUDIO_INPUT_TRAIN, AUDIO_INPUT_TEST]\n",
    "\n",
    "filenames = []\n",
    "for directory in directories:\n",
    "    filenames.extend([\n",
    "        filename for filename in os.listdir(directory) \n",
    "        if filename.lower().endswith(FILETYPES)\n",
    "])\n",
    "    \n",
    "# Process train dataset\n",
    "train_audio, train_sr, train_stft_data, train_audio_labels = process_audio_dataset(\n",
    "    AUDIO_INPUT_TRAIN, \n",
    "    FILETYPES\n",
    ")\n",
    "\n",
    "# Process test dataset\n",
    "test_audio, test_sr, test_stft_data, test_audio_labels = process_audio_dataset(\n",
    "    AUDIO_INPUT_TEST, \n",
    "    FILETYPES\n",
    ")\n",
    "\n",
    "# test\n",
    "print(f\"Processed {len(train_stft_data)} train audio files\")\n",
    "print(f\"Processed {len(test_stft_data)} test audio files\")\n",
    "\n",
    "for file in filenames:\n",
    "    if file in train_stft_data:\n",
    "        plot_audio_stft(train_audio[file], train_sr[file], train_stft_data[file], os.path.splitext(file)[0])\n",
    "    else:\n",
    "        plot_audio_stft(test_audio[file], test_sr[file], test_stft_data[file], os.path.splitext(file)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    \n",
    "    if file in train_stft_data:\n",
    "        filename_without_ext = os.path.splitext(file)[0]\n",
    "        filepath = os.path.join(STFT_PLOTS, filename_without_ext + '.png')\n",
    "        spatiotemporal_gen(filepath, train_audio[file], train_sr[file], os.path.splitext(file)[0])\n",
    "    else:\n",
    "        spatiotemporal_gen(filepath, test_audio[file], test_sr[file], os.path.splitext(file)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    /home/arafat/work/thesis/env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 images\n",
      "Unique labels: ['52441.wav', '7068.wav', '76089.wav', '7913.wav', '97331.wav', '99500.wav']\n",
      "Label mapping: {'52441.wav': 0, '7068.wav': 1, '76089.wav': 2, '7913.wav': 3, '97331.wav': 4, '99500.wav': 5}\n",
      "Loaded 3 images\n",
      "Unique labels: ['20571.wav', '7067.wav', '97317.wav']\n",
      "Label mapping: {'20571.wav': 0, '7067.wav': 1, '97317.wav': 2}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100.\u001b[39m\u001b[38;5;241m*\u001b[39mcorrect\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Run training and evaluation\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m evaluate(net, test_loader)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Optional: Save the model\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch.save(net.state_dict(), 'snn_small_dataset_model.pth')\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Print dataset information\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 193\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, num_epochs)\u001b[0m\n\u001b[1;32m    190\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, targets)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Compute accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class CustomLabeledImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for loading images with provided labels\n",
    "        \n",
    "        Args:\n",
    "            folder_path (string): Path to the folder with images\n",
    "            labels (list): List of labels corresponding to images\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Supported image extensions\n",
    "        img_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.gif']\n",
    "        \n",
    "        # Scan directory for image files\n",
    "        filenames = os.listdir(folder_path)\n",
    "        \n",
    "        # Match images with labels\n",
    "        for filename in filenames:\n",
    "            if any(filename.lower().endswith(ext) for ext in img_extensions):\n",
    "                # Find corresponding label\n",
    "                matching_labels = [label for label in labels if filename.split('.')[0] in str(label)]\n",
    "                \n",
    "                if matching_labels:\n",
    "                    self.image_paths.append(os.path.join(folder_path, filename))\n",
    "                    self.labels.append(matching_labels[0])\n",
    "        \n",
    "        # Convert labels to categorical\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.labels = [self.label_to_index[label] for label in self.labels]\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"Loaded {len(self.image_paths)} images\")\n",
    "        print(f\"Unique labels: {unique_labels}\")\n",
    "        print(f\"Label mapping: {self.label_to_index}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2  # Adjusted for small dataset\n",
    "data_path = './data/spatiotemporal_plots'  # Update this to your actual path\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transformations with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to consistent size\n",
    "    transforms.RandomRotation(10),  # Light data augmentation\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalization\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "# TODO: Replace these with your actual labels from preprocessing\n",
    "train_dataset = CustomLabeledImageDataset(\n",
    "    os.path.join(data_path, 'train'), \n",
    "    train_audio_labels,  # Use your preprocessed labels \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = CustomLabeledImageDataset(\n",
    "    os.path.join(data_path, 'test'), \n",
    "    test_audio_labels,  # Use your preprocessed labels\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Number of classes (inferred from dataset)\n",
    "num_classes = len(train_dataset.label_to_index)\n",
    "\n",
    "# Simplified Spiking Neural Network Model\n",
    "class SmallDatasetSpikingNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Leaky neuron parameters\n",
    "        beta = 0.5\n",
    "        spike_grad = surrogate.sigmoid(slope=10)\n",
    "\n",
    "        # Simplified network architecture\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Reduced fully connected layers\n",
    "        self.fc1 = nn.Linear(8 * 14 * 14, 32)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tracking membrane potential\n",
    "        spk_out_list = []\n",
    "        \n",
    "        # Convolution and spiking layer\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x, mem1 = self.lif1(x)\n",
    "        spk_out_list.append(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x, mem2 = self.lif2(x)\n",
    "        spk_out_list.append(x)\n",
    "        \n",
    "        # Final layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, spk_out_list\n",
    "\n",
    "# Initialize the model\n",
    "net = SmallDatasetSpikingNet(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Training Loop\n",
    "def train(net, train_loader, num_epochs):\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = net(data)\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = output.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Loss: {total_loss/len(train_loader):.4f}, '\n",
    "              f'Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(net, test_loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            output, _ = net(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    print(f'Test Accuracy: {100.*correct/total:.2f}%')\n",
    "\n",
    "# Run training and evaluation\n",
    "train(net, train_loader, num_epochs)\n",
    "evaluate(net, test_loader)\n",
    "\n",
    "# Optional: Save the model\n",
    "# torch.save(net.state_dict(), 'snn_small_dataset_model.pth')\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Total training images: {len(train_dataset)}\")\n",
    "print(f\"Total test images: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Label mapping:\", train_dataset.label_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
