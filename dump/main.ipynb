{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Author: Shakil Mahmud Arafat, EEE AUST\\n    Date last updated: 28 Nov, 2024\\n    Description: \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Author: Shakil Mahmud Arafat, EEE AUST\n",
    "    Date last updated: 28 Nov, 2024\n",
    "    Description: \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project setup and configs\n",
    "FILETYPES = (\".ogg\", \".mp3\", \"wav\")\n",
    "AUDIO_INPUT_TEST = \"./data/audio_dataset/test\"\n",
    "AUDIO_INPUT_TRAIN = \"./data/audio_dataset/train\"\n",
    "STFT_PLOTS = \"./data/stft_plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd  # Import pandas for CSV reading\n",
    "\n",
    "def audio_to_stft(dir):\n",
    "    \"\"\"\n",
    "    Reads audio files from a directory, performs STFT on each file,\n",
    "    and returns dictionaries containing STFT matrices, audio data with\n",
    "    sampling rates, and audio type tags from corresponding CSV files.\n",
    "\n",
    "    Args:\n",
    "        dir (str): Directory containing audio files and CSV files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three dictionaries:\n",
    "               - stft_data: keys are filenames and values are the corresponding STFT matrices.\n",
    "               - audio_data: keys are filenames and values are tuples of (audio_data, sampling_rate).\n",
    "               - audio_tags: keys are filenames and values are the audio type tags from the CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    stft_data = {}\n",
    "    audio_data = {}\n",
    "    audio_tags = {}\n",
    "\n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith(FILETYPES):\n",
    "            file_path = os.path.join(dir, filename)\n",
    "            audio, sr = librosa.load(file_path)\n",
    "            stft = np.abs(librosa.stft(audio))\n",
    "            stft_data[filename] = stft\n",
    "            audio_data[filename] = (audio, sr)\n",
    "\n",
    "            # Extract audio type tag from corresponding CSV file using pandas\n",
    "            csv_filename = os.path.splitext(filename)[0] + \".csv\"\n",
    "            csv_filepath = os.path.join(dir, csv_filename)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(csv_filepath, names=['startTime', 'endTime', 'quantity', 'label'])  # Read CSV with pandas\n",
    "                audio_tags[filename] = df.iloc[0,3]  # label on 4th column\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: CSV file {csv_filename} not found.\")\n",
    "                audio_tags[filename] = None\n",
    "            except IndexError:\n",
    "                print(f\"Warning: CSV file {csv_filename} label not found.\")\n",
    "                audio_tags[filename] = None\n",
    "\n",
    "    return stft_data, audio_data, audio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_audio_stft(audio, sr, stft, output_filename):\n",
    "    \"\"\"\n",
    "    Plots the waveform and STFT of a single audio file and saves the plot.\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): The audio time series.\n",
    "        sr (int): The sampling rate of the audio file.\n",
    "        stft (np.ndarray): The STFT matrix of the audio file.\n",
    "        output_filename (str): The name of the output file for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate time axis for both plots\n",
    "    time_axis = np.arange(len(audio)) / sr  # Time axis for waveform\n",
    "    stft_time_axis = np.arange(stft.shape[1]) * (\n",
    "        stft.shape[1] / sr\n",
    "    )  # Time axis for STFT\n",
    "\n",
    "    # Create a figure with two rows and shared x-axis\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "    # Plot the waveform in the first row\n",
    "    axs[0].plot(time_axis, audio)\n",
    "    axs[0].set_ylabel(\"Amplitude\")\n",
    "    axs[0].set_title(\"Waveform\")\n",
    "\n",
    "    # Plot the STFT in the second row\n",
    "    librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(stft, ref=np.max),\n",
    "        sr=sr,\n",
    "        x_axis=\"time\",\n",
    "        y_axis=\"hz\",\n",
    "        cmap=\"viridis\",\n",
    "        ax=axs[1],\n",
    "    )\n",
    "    axs[1].set_xlabel(\"Time (s)\")\n",
    "    axs[1].set_ylabel(\"Frequency (Hz)\")\n",
    "    axs[1].set_title(\"STFT\")\n",
    "\n",
    "    # Layout so plots do not overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(STFT_PLOTS, f\"{output_filename}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import snntorch.spikegen as spikegen\n",
    "\n",
    "class PreProcessAudioDataset(Dataset):\n",
    "    def __init__(self, stft_data, audio_labels, encoding_type='rate', num_steps=50): \n",
    "        self.stft_data = stft_data\n",
    "        self.audio_labels = audio_labels\n",
    "        self.filenames = list(stft_data.keys())\n",
    "        self.label_map = {\"gun_shot\": 0, \"dog_bark\": 1, \"children_playing\": 2}\n",
    "        self.encoding_type = encoding_type\n",
    "        self.num_steps = num_steps  # Number of time steps for encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        stft = self.stft_data[filename]\n",
    "            \n",
    "        # Pad/truncate in the frequency dimension\n",
    "        num_freq_bins = stft.shape[1]\n",
    "        max_freq_bins = 100\n",
    "        if num_freq_bins < max_freq_bins:\n",
    "            pad_amount = max_freq_bins - num_freq_bins\n",
    "            stft = np.pad(stft, ((0, 0), (0, pad_amount)), 'constant')\n",
    "        elif num_freq_bins > max_freq_bins:\n",
    "            stft = stft[:, :max_freq_bins] \n",
    "            \n",
    "        # Min-Max Normalization (before tensor conversion)\n",
    "        min_val = stft.min()\n",
    "        max_val = stft.max()\n",
    "        stft = (stft - min_val) / (max_val - min_val) \n",
    "\n",
    "        # Convert to tensor\n",
    "        stft = torch.from_numpy(stft).float()  \n",
    "\n",
    "        # Spike Encoding\n",
    "        if self.encoding_type == 'rate':\n",
    "            spike_train = spikegen.rate(stft, num_steps=self.num_steps)\n",
    "        elif self.encoding_type == 'latency':\n",
    "            spike_train = spikegen.latency(stft, num_steps=self.num_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid encoding_type. Choose 'rate' or 'latency'\")\n",
    "\n",
    "        label = self.label_map[self.audio_labels[filename]] \n",
    "        return stft, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7068.wav gun_shot (1025, 137)\n",
      "76089.wav gun_shot (1025, 44)\n",
      "52441.wav dog_bark (1025, 3208)\n",
      "7913.wav dog_bark (1025, 358)\n",
      "97331.wav children_playing (1025, 9740)\n",
      "97331.mp3 children_playing (1025, 9739)\n",
      "99500.wav children_playing (1025, 4878)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Loading and spectogram\n",
    "\"\"\"\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Loads the data, performs stft and returns the stft, audio and labels\n",
    "train_stft_data, train_audio_data, train_audio_label = audio_to_stft(AUDIO_INPUT_TRAIN)\n",
    "test_stft_data, test_audio_data, test_audio_label = audio_to_stft(AUDIO_INPUT_TEST)\n",
    "\n",
    "\n",
    "# Plots the audio and stft spectogram\n",
    "for filename in os.listdir(AUDIO_INPUT_TRAIN):\n",
    "    # print(filename)\n",
    "    if filename in train_stft_data:\n",
    "        (audio, sr) = train_audio_data[filename]\n",
    "        print(filename, train_audio_label[filename], train_stft_data[filename].shape)\n",
    "        # plot_audio_stft(audio, sr, train_stft_data[filename], os.path.splitext(filename)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "train_dataset = PreProcessAudioDataset(\n",
    "    train_stft_data, \n",
    "    train_audio_label, \n",
    "    encoding_type='rate',  # or 'rate'\n",
    "    num_steps=100  # Adjust the number of time steps as needed\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dataset = PreProcessAudioDataset(\n",
    "    test_stft_data, \n",
    "    test_audio_label, \n",
    "    encoding_type='rate',\n",
    "    num_steps=100 \n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([1025, 100])\n",
      "torch.Size([2, 1025, 100])\n",
      "torch.Size([2, 102500])\n"
     ]
    }
   ],
   "source": [
    "# dataset shape\n",
    "for train, label in train_dataset:\n",
    "    print(train.shape)\n",
    "    # break\n",
    "# dataloader and input shape\n",
    "for spike_train, label in train_dataloader:\n",
    "    print(spike_train.shape)  # shape before flattening\n",
    "    x = spike_train.view(spike_train.size(0), -1)  # flattened\n",
    "    print(x.shape)  # shape after flattening \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(device)\n",
    "# Network Architecture\n",
    "num_inputs = 1025*100\n",
    "num_hidden = 100\n",
    "num_outputs = 3\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 2\n",
    "beta = 0.95\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        elf.fc3 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(2, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer(\n",
    "    data, targets, epoch,\n",
    "    counter, iter_counter,\n",
    "        loss_hist, test_loss_hist, test_data, test_targets):\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x51250 and 102500x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m test_targets \u001b[38;5;241m=\u001b[39m test_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Test set forward pass\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m test_spk, test_mem \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Test set loss\u001b[39;00m\n\u001b[1;32m     48\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 37\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     cur1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     spk1, mem1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlif1(cur1, mem1)\n\u001b[1;32m     39\u001b[0m     cur2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(spk1)\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/thesis/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x51250 and 102500x100)"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "batch_size = 2\n",
    "dtype = torch.float\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_dataloader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_dataloader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer(\n",
    "                    data, targets, epoch,\n",
    "                    counter, iter_counter,\n",
    "                    loss_hist, test_loss_hist,\n",
    "                    test_data, test_targets)\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
